{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDINETから「主な相手先別の販売実績」を抽出する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: edinet_xbrl in c:\\users\\hisan\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: python-xbrl in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from edinet_xbrl) (1.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from edinet_xbrl) (2.29.0)\n",
      "Requirement already satisfied: pytest in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from python-xbrl->edinet_xbrl) (7.3.1)\n",
      "Requirement already satisfied: pep8 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from python-xbrl->edinet_xbrl) (1.7.1)\n",
      "Requirement already satisfied: six in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from python-xbrl->edinet_xbrl) (1.16.0)\n",
      "Requirement already satisfied: marshmallow in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from python-xbrl->edinet_xbrl) (3.20.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from python-xbrl->edinet_xbrl) (4.9.2)\n",
      "Requirement already satisfied: ordereddict in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from python-xbrl->edinet_xbrl) (1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from python-xbrl->edinet_xbrl) (4.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from requests->edinet_xbrl) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from requests->edinet_xbrl) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from requests->edinet_xbrl) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from requests->edinet_xbrl) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from beautifulsoup4->python-xbrl->edinet_xbrl) (2.4)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from marshmallow->python-xbrl->edinet_xbrl) (23.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from pytest->python-xbrl->edinet_xbrl) (2.0.1)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from pytest->python-xbrl->edinet_xbrl) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from pytest->python-xbrl->edinet_xbrl) (0.4.6)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from pytest->python-xbrl->edinet_xbrl) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from pytest->python-xbrl->edinet_xbrl) (1.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install edinet_xbrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from edinet_xbrl.edinet_xbrl_parser import EdinetXbrlParser\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-02 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-03 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-04 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-05 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-06 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-07 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-08 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-09 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-10 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-11 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-12 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-13 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-14 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-15 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-16 00:00:00: 2 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-17 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-18 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-19 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-20 00:00:00: 1 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-21 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-22 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-23 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-24 00:00:00: 2 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-25 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-26 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-01-27 00:00:00: 4 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-28 00:00:00: 6 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-29 00:00:00: 9 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-30 00:00:00: 19 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-31 00:00:00: 10 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-01 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-02 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-03 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-04 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-05 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-06 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-07 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-08 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-09 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-10 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-11 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-12 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-13 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-14 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-15 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-16 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-17 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-18 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-19 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-20 00:00:00: 6 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-21 00:00:00: 5 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-22 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-23 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-24 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-02-25 00:00:00: 3 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-26 00:00:00: 6 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-27 00:00:00: 22 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-28 00:00:00: 15 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-29 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-01 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-02 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-03 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-04 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-05 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-06 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-07 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-08 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-09 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-10 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-11 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-12 00:00:00: 2 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-13 00:00:00: 5 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-14 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-15 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-16 00:00:00: 2 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-17 00:00:00: 2 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-18 00:00:00: 2 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-19 00:00:00: 12 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-20 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-21 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-22 00:00:00: 有価証券報告書の提出情報がありません。\n",
      "2020-03-23 00:00:00: 14 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-24 00:00:00: 16 件の有価証券報告書が抽出されました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "END_POINT = 'https://disclosure.edinet-fsa.go.jp/api/v1'\n",
    "submission_info_endpoint = f'{END_POINT}/documents.json'\n",
    "\n",
    "# 最終的な返り値のdataframe\n",
    "output_df = pd.DataFrame(columns=['相手先', '前連結_金額（百万円）', '前連結_割合（%）', '当連結_金額（百万円）', '当連結_割合（%）'])\n",
    "\n",
    "# 開始日と終了日を設定\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2020, 3, 31)\n",
    "\n",
    "# 1日ごとにデータを取得\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    submission_request_parameters = {\n",
    "        'date': current_date.strftime('%Y-%m-%d'),\n",
    "        'type': 2\n",
    "    }\n",
    "    submission_info_response = requests.get(submission_info_endpoint, params=submission_request_parameters)\n",
    "    submission_info_json = submission_info_response.json()\n",
    "\n",
    "    \n",
    "    # 取得したデータを処理するコードをここに追加\n",
    "\n",
    "    raw_submission_info_df = pd.DataFrame(submission_info_json['results'])\n",
    "    # raw_submission_info_df.columns\n",
    "    \n",
    "    if any(col not in raw_submission_info_df.columns \n",
    "           for col in ['docID', 'edinetCode', 'secCode', 'filerName', 'docDescription']):\n",
    "        print(f'{current_date}: 有価証券報告書の提出情報がありません。')\n",
    "        current_date += timedelta(days=1)\n",
    "        continue\n",
    "\n",
    "    # 重要なカラムに絞る\n",
    "    submission_info_df = raw_submission_info_df[['docID', 'edinetCode', 'secCode', 'filerName', 'docDescription']]\n",
    "    # submission_info_df.head()\n",
    "    \n",
    "    # 有価証券報告書の情報を抽出する。\n",
    "    securities_report_infos = []\n",
    "    for i, row in submission_info_df.iterrows():\n",
    "        doc_desc = row['docDescription']\n",
    "        doc_seccode = row['secCode']\n",
    "        \n",
    "        if doc_desc is None:\n",
    "            continue\n",
    "        \n",
    "        if doc_seccode is None:\n",
    "            continue\n",
    "        \n",
    "        if ('有価証券報告書' in doc_desc) and ('受益証券' not in doc_desc) and ('訂正' not in doc_desc) and ('外国投資証券' not in doc_desc):\n",
    "            row_to_dataframe = pd.DataFrame([row])\n",
    "            securities_report_infos.append(row_to_dataframe)\n",
    "\n",
    "    if len(securities_report_infos) == 0:\n",
    "        print(f'{current_date}: 有価証券報告書の提出情報がありません。')\n",
    "        current_date += timedelta(days=1)\n",
    "        continue\n",
    "    else:\n",
    "        print(f'{current_date}: {len(securities_report_infos)} 件の有価証券報告書が抽出されました。')\n",
    "        securities_report_info_df = pd.concat(securities_report_infos)\n",
    "\n",
    "    for docID in securities_report_info_df['docID']:\n",
    "        \n",
    "        label = securities_report_info_df[securities_report_info_df['docID'] == docID]\n",
    "        document_endpoint = f'{END_POINT}/documents/{docID}'\n",
    "        document_request_parameters = {\n",
    "            'type': 1\n",
    "        }\n",
    "        document_response = requests.get(document_endpoint, document_request_parameters)\n",
    "        \n",
    "        # まず、返ってきたデータを zip 形式で保存する。\n",
    "        zip_file_full_path = f'D:/EDINET_DATA/{docID}.zip'\n",
    "        with open(zip_file_full_path, 'wb') as f:\n",
    "            for chunk in document_response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        # zip ファイルを解凍する\n",
    "        output_dir = f'D:/EDINET_DATA/{docID}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_file_full_path) as zip_f:\n",
    "            zip_f.extractall(output_dir)\n",
    "        \n",
    "        # xbrl ファイルを発見する\n",
    "        # PublicDoc 内に格納されている xbrl ファイルが分析対象となるファイルである。\n",
    "        xbrl_expression = f'D:/EDINET_DATA/{docID}/**/PublicDoc/**/*.xbrl'\n",
    "        xbrl_paths = glob(xbrl_expression, recursive=True)\n",
    "        \n",
    "        # print(xbrl_paths)\n",
    "        \n",
    "        parser = EdinetXbrlParser()\n",
    "        # Step2で特定した XBRL ファイルのパスを選択\n",
    "        if xbrl_paths == []:\n",
    "            continue\n",
    "        else:\n",
    "            xbrl_path = xbrl_paths[0]\n",
    "        parsed_xbrl = parser.parse_file(xbrl_path)\n",
    "        \n",
    "        \n",
    "        # 経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析 [テキストブロック]の取得\n",
    "        key = 'jpcrp_cor:ManagementAnalysisOfFinancialPositionOperatingResultsAndCashFlowsTextBlock'\n",
    "        context_ref = 'FilingDateInstant'\n",
    "        extracted_data = parsed_xbrl.get_data_by_context_ref(key, context_ref)\n",
    "        \n",
    "        if extracted_data is None:\n",
    "            continue\n",
    "        else:\n",
    "            ManagementAnalysis = extracted_data.get_value()\n",
    "    \n",
    "        def extract_paragraph_and_following_table(html_code, target_text):\n",
    "            # BeautifulSoupを使ってパース\n",
    "            soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "            # 特定の文字列を含むパラグラフを抽出\n",
    "            target_paragraphs = [p for p in soup.find_all('p') if target_text in p.text]\n",
    "\n",
    "            # 対応する表を抽出\n",
    "            tables = []\n",
    "            for paragraph in target_paragraphs:\n",
    "                table = paragraph.find_next('table')\n",
    "                if table:\n",
    "                    tables.append(table)\n",
    "\n",
    "            return target_paragraphs, tables\n",
    "    \n",
    "    \n",
    "        def extract_table_content(table):\n",
    "            # テーブルの行を取得\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # 各行のデータを取得\n",
    "            table_data = []\n",
    "            for row in rows:\n",
    "                cols = row.find_all(['th', 'td'])\n",
    "                cols = [col.text.strip() for col in cols]\n",
    "                table_data.append(cols)\n",
    "\n",
    "            return table_data\n",
    "\n",
    "        html_code = ManagementAnalysis\n",
    "\n",
    "        # 特定の文字列を含むパラグラフとその次に続く表を抽出する関数を呼び出す\n",
    "        target_text = \"主な相手先別\"\n",
    "        paragraphs, tables = extract_paragraph_and_following_table(html_code, target_text)\n",
    "        \n",
    "        for p in paragraphs:\n",
    "            if '主な相手先別' in p.get_text():\n",
    "                supplier_text = p.get_text()\n",
    "        \n",
    "        if 'supplier_text' not in globals():\n",
    "            continue\n",
    "        \n",
    "        if '省略' in supplier_text:\n",
    "            table_content = []\n",
    "        else:\n",
    "            # 抽出したテーブルの内容を表示\n",
    "            if tables:\n",
    "                for table in tables:\n",
    "                    table_content = extract_table_content(table)\n",
    "            else:\n",
    "                table_content = []\n",
    "        \n",
    "        # テーブルの内容をデータフレームに変換\n",
    "        additional_columns = ['相手先', '前連結_金額（百万円）', '前連結_割合（%）', '当連結_金額（百万円）', '当連結_割合（%）']\n",
    "\n",
    "        result = [item for item in table_content if len(item) == 5]\n",
    "        \n",
    "        # データフレームを結合\n",
    "        for element in result:\n",
    "            label[additional_columns] = element\n",
    "            output_df = pd.concat([label, output_df], axis=0)\n",
    "\n",
    "        # display(output_df)\n",
    "        \n",
    "        # 使用が終わったファイルを削除する\n",
    "        shutil.rmtree(output_dir)  # ディレクトリごと削除\n",
    "        os.remove(zip_file_full_path)  # zipファイルを削除\n",
    "\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_dfの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>相手先</th>\n",
       "      <th>前連結_金額（百万円）</th>\n",
       "      <th>前連結_割合（%）</th>\n",
       "      <th>当連結_金額（百万円）</th>\n",
       "      <th>当連結_割合（%）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [相手先, 前連結_金額（百万円）, 前連結_割合（%）, 当連結_金額（百万円）, 当連結_割合（%）]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "excelの掃き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hisan\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211: FutureWarning: the 'encoding' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'encoding'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "output_df.to_excel('C:/Users/hisan/OneDrive/デスクトップ/edinet_xbrl/output-data/yuho_supplychain_20200103.xlsx', index=False ,encoding=\"shift-jis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
